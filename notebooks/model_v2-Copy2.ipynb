{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "inside-sacramento",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "informational-peter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('enriched-extended-issues-with-relationship.json','r') as f:\n",
    "    data = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "canadian-promise",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tweet = TweetTokenizer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def clean_text(x):\n",
    "#     x = re.sub(r'[^A-Za-z0-9 ]+', '', x)\n",
    "    text = nltk.tokenize.word_tokenize(x)\n",
    "    text_clean = \" \".join([i.lower() for i in text if i not in string.punctuation and i not in stopwords])\n",
    "    return text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fresh-payroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = ['to','the','for', 'this','a', 'and','of','is','we','be','pdd','com', 'puzzle','in','on','by',\n",
    "             'has', 'was','me', 'http','html','about','yegor','www','from','java','github','new','have','https',\n",
    "              'code','will','instead','any','don','text','task','more','when','you', 'problem','tickets',\"n't\",\n",
    "              '``'\n",
    "            ]\n",
    "\n",
    "def get_row(issue):\n",
    "    def _(f):\n",
    "        return issue[f] if f in issue else None\n",
    "    \n",
    "    def _t(f1, f2):\n",
    "        return (parse(_(f2)) - parse(_(f1))).total_seconds() if _(f1) and _(f2) else None \n",
    "    \n",
    "    def __(f1, f2):\n",
    "        return _(f1)[f2] if _(f1) and f2 in _(f1) else None\n",
    "    \n",
    "    def _l():\n",
    "        return [label['name'] for label in _('labels')] if _('labels') else []\n",
    "    \n",
    "    def clean(x):\n",
    "        return ' '.join([word for word in word_tokenize(x) if word not in stop_words])\n",
    "    \n",
    "    title = _('title')\n",
    "    body = _('body')\n",
    "    issue_id = _('id')\n",
    "    child_id = __('parentIssue','id')\n",
    "    text = f'{clean(title)} {clean(body)}'\n",
    "    url = _('issueLink')\n",
    "    closed_at = _('closed_at')\n",
    "    number_of_additions = __('code','additions')\n",
    "    if url:\n",
    "        url_part = url.split('/')\n",
    "        repo = url_part[3] + '/'+ url_part[4]\n",
    "    else:\n",
    "        repo = None\n",
    "    if text and not text.isspace() and repo and closed_at and number_of_additions:  \n",
    "        return [issue_id, child_id, clean_text(text), repo, closed_at,number_of_additions]\n",
    "    else:\n",
    "        return [None,None,None,None,None,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "legal-graphics",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_data = []\n",
    "\n",
    "overall_data.extend([get_row(issue) for issue in data])\n",
    "df = pd.DataFrame(overall_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "swiss-administrator",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['iid', 'pid', 'text','repo', 'closed_at', 'additions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "heated-causing",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['text'])\n",
    "df = df.drop(['pid'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beginning-christopher",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df.sample(n = 500)\n",
    "df_train_index = sample.index\n",
    "df_train = df.drop(df_train_index)\n",
    "df_test = sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "gross-zealand",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "intensive-store",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_window(puzzles,n = 5):\n",
    "    results = []\n",
    "    for i in range(len(puzzles)):\n",
    "        res = [puzzles[i][0], [puzzles[j][0] for j in range(i + 1, min((i + n + 1), len(puzzles)))]]\n",
    "        if len(res[1]) >= 1:\n",
    "            results.append(res)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "formed-bundle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO need to add negative samples, they are based on the existing at that moment of time task that wasn't selected or was abandoned\n",
    "results = []\n",
    "for idx,row in df_train.groupby(['repo']).agg(lambda x: list(x)).iterrows():\n",
    "    if len(row['iid'])>= 2:\n",
    "        text = row['text']\n",
    "        close_dates = [parse(date) for date in  row['closed_at']]\n",
    "        res = sorted(list(zip(text, close_dates)), key = lambda x: x[1])\n",
    "        res = create_window(res)\n",
    "        results.extend(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dramatic-outside",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = []\n",
    "for row in results:\n",
    "    result_row = []\n",
    "    parent = row[0]\n",
    "    childrens = row[1]\n",
    "    for i in range(len(childrens)):\n",
    "        weight = (len(childrens) - i) / len(childrens)\n",
    "        result_row.append([parent, childrens[i], weight])\n",
    "    final_results.extend(result_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "activated-appearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "taken-small",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test format the 2-45ffc6ab 2 resolved //github...</td>\n",
       "      <td>solve lint errors 152 error currently the 1-b4...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2out.h:13-13 place classes files the 5-03512da...</td>\n",
       "      <td>2out.h:15-15 move tstsimple tstsimple.cpp the ...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2out.h:13-13 place classes files the 5-03512da...</td>\n",
       "      <td>2out.h:32-32 move tstsuite tstsuite.cpp the 8-...</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2out.h:13-13 place classes files the 5-03512da...</td>\n",
       "      <td>2out.h:56-56 move assertionequal asequal.cpp t...</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2out.h:13-13 place classes files the 5-03512da...</td>\n",
       "      <td>textreporttest.h:16-16 textreporttest incapsul...</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12111</th>\n",
       "      <td>config.yml:2-3 integrate ci trigger push ... t...</td>\n",
       "      <td>applogger.tsx:44 bsod shown critical errors th...</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12112</th>\n",
       "      <td>config.yml:2-3 integrate ci trigger push ... t...</td>\n",
       "      <td>index.tsx:33 move custom middlewares the 138-a...</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12113</th>\n",
       "      <td>applogger.tsx:16 provide logger component catc...</td>\n",
       "      <td>applogger.tsx:44 bsod shown critical errors th...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12114</th>\n",
       "      <td>applogger.tsx:16 provide logger component catc...</td>\n",
       "      <td>index.tsx:33 move custom middlewares the 138-a...</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12115</th>\n",
       "      <td>applogger.tsx:44 bsod shown critical errors th...</td>\n",
       "      <td>index.tsx:33 move custom middlewares the 138-a...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12116 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0  \\\n",
       "0      test format the 2-45ffc6ab 2 resolved //github...   \n",
       "1      2out.h:13-13 place classes files the 5-03512da...   \n",
       "2      2out.h:13-13 place classes files the 5-03512da...   \n",
       "3      2out.h:13-13 place classes files the 5-03512da...   \n",
       "4      2out.h:13-13 place classes files the 5-03512da...   \n",
       "...                                                  ...   \n",
       "12111  config.yml:2-3 integrate ci trigger push ... t...   \n",
       "12112  config.yml:2-3 integrate ci trigger push ... t...   \n",
       "12113  applogger.tsx:16 provide logger component catc...   \n",
       "12114  applogger.tsx:16 provide logger component catc...   \n",
       "12115  applogger.tsx:44 bsod shown critical errors th...   \n",
       "\n",
       "                                                       1         2  \n",
       "0      solve lint errors 152 error currently the 1-b4...  1.000000  \n",
       "1      2out.h:15-15 move tstsimple tstsimple.cpp the ...  1.000000  \n",
       "2      2out.h:32-32 move tstsuite tstsuite.cpp the 8-...  0.800000  \n",
       "3      2out.h:56-56 move assertionequal asequal.cpp t...  0.600000  \n",
       "4      textreporttest.h:16-16 textreporttest incapsul...  0.400000  \n",
       "...                                                  ...       ...  \n",
       "12111  applogger.tsx:44 bsod shown critical errors th...  0.666667  \n",
       "12112  index.tsx:33 move custom middlewares the 138-a...  0.333333  \n",
       "12113  applogger.tsx:44 bsod shown critical errors th...  1.000000  \n",
       "12114  index.tsx:33 move custom middlewares the 138-a...  0.500000  \n",
       "12115  index.tsx:33 move custom middlewares the 138-a...  1.000000  \n",
       "\n",
       "[12116 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "innocent-bryan",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "txt1 = np.random.randint(0, high=12109, size=3000)\n",
    "txt2 = np.random.randint(0, high=12109, size=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "brutal-istanbul",
   "metadata": {},
   "outputs": [],
   "source": [
    "fr2 = list(zip(df3.iloc[txt1][0].values, df3.iloc[txt1][1].values, np.zeros(3000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "pointed-reynolds",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "words = nltk.tokenize.word_tokenize(' '.join(df['text'].values))\n",
    "fdist = FreqDist(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "available-narrative",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(list(fdist.keys()))\n",
    "embedding_dim = 30 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "exposed-behalf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "class RecommenderNet(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(RecommenderNet, self).__init__()\n",
    "        self.e = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim * 50 *  2, 512)\n",
    "        self.hidden = nn.Sequential(\n",
    "                                 nn.Linear(512,256),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(0.2),\n",
    "                                 nn.Linear(256, 128),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(0.2)\n",
    "        )\n",
    "        self.linear2 = nn.Linear(128, 1)\n",
    "        \n",
    "    def forward(self, s1, s2):\n",
    "        e1 = self.e(s1)\n",
    "        e2 = self.e(s2)\n",
    "        e3 = torch.cat([e1, e2], 1)\n",
    "        ersh = e3.shape\n",
    "        e3 = torch.reshape(e3, (ersh[0], ersh[1] * ersh[2]))\n",
    "        out = F.relu(self.linear1(e3))\n",
    "        out = self.hidden(out)\n",
    "        out = F.sigmoid(self.linear2(out))\n",
    "        return out\n",
    "    \n",
    "    def _init(self):\n",
    "        def init(m):\n",
    "            if type(m) == nn.Linear:\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                m.bias.data.fill_(0.01)\n",
    "        self.u.weight.data.uniform_(-0.05, 0.05)\n",
    "        self.m.weight.data.uniform_(-0.05, 0.05)\n",
    "        self.hidden.apply(init)\n",
    "        init(self.fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "sharing-migration",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_LAUNCH_BLOCKING=1\n",
    "net = RecommenderNet(vocab_size, embedding_dim)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "twenty-weapon",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.3, patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "monetary-rachel",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(fdist.keys())\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "incredible-words",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = final_results + fr2\n",
    "train_index = int(len(final_results) * 0.7)\n",
    "import random\n",
    "\n",
    "random.shuffle(final_results)\n",
    "\n",
    "# train = final_results[:train_index]\n",
    "# test = final_results[train_index: ]\n",
    "# s1, s2, r = zip(*train)\n",
    "train = final_results\n",
    "# test = final_results[train_index: ]\n",
    "s1, s2, r = zip(*train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "heard-foster",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15116"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bored-rider",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = []\n",
    "\n",
    "batch_size = 100\n",
    "n_samples = len(train)\n",
    "\n",
    "#Create batches\n",
    "for i in range(0, n_samples, batch_size):\n",
    "    limit = min(i + batch_size, n_samples)\n",
    "    s1_batch = []\n",
    "    for s in s1[i:limit]:\n",
    "        sentense = nltk.tokenize.word_tokenize(s)\n",
    "        s1_batch.append(torch.tensor([word_to_ix[sentense[w]] if w < len(sentense) else 0 for w in range(50)]))\n",
    "    s2_batch = []\n",
    "    for s in s2[i:limit]:\n",
    "        sentense = nltk.tokenize.word_tokenize(s)\n",
    "        s2_batch.append(torch.tensor([word_to_ix[sentense[w]] if w < len(sentense) else 0 for w in range(50)]))\n",
    "    r_batch = torch.tensor(r[i:limit], dtype=torch.float)\n",
    "    batches.append((torch.stack(s1_batch), torch.stack(s2_batch), r_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "unnecessary-costa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/152 [00:00<?, ?it/s]/home/yaroslav/miniconda3/envs/juplab/lib/python3.9/site-packages/torch/nn/functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "100%|██████████| 152/152 [00:03<00:00, 49.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 0 = 0.11400288343429565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 152/152 [00:03<00:00, 49.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 1 = 0.11113207042217255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 152/152 [00:03<00:00, 49.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 2 = 0.11180835217237473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 152/152 [00:03<00:00, 48.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 3 = 0.10453560948371887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 152/152 [00:03<00:00, 49.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 4 = 0.0955808088183403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 152/152 [00:03<00:00, 49.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 5 = 0.08924049139022827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 152/152 [00:03<00:00, 50.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 6 = 0.08122327923774719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 152/152 [00:03<00:00, 50.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 7 = 0.06955324858427048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 152/152 [00:03<00:00, 49.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 8 = 0.044311027973890305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 152/152 [00:03<00:00, 50.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 9 = 0.018404267728328705\n",
      "Last Loss = 0.018404267728328705\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  train_loss = 0\n",
    "  for s1_batch, s2_batch, rates_batch in tqdm(batches):\n",
    "    net.zero_grad()\n",
    "    out = net(s1_batch.to(device), s2_batch.to(device)).squeeze()\n",
    "    loss = criterion(rates_batch.to(device), out)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss += loss\n",
    "  scheduler.step(loss)\n",
    "  print(\"Loss at epoch {} = {}\".format(epoch, loss.item()))\n",
    "print(\"Last Loss = {}\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "embedded-quebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "results2 = []\n",
    "for idx,row in df_test.groupby(['repo']).agg(lambda x: list(x)).iterrows():\n",
    "    if len(row['iid'])>= 2:\n",
    "        text = row['text']\n",
    "        close_dates = [parse(date) for date in  row['closed_at']]\n",
    "        res = sorted(list(zip(text, close_dates)), key = lambda x: x[1])\n",
    "        res = create_window(res)\n",
    "        results2.extend(res)\n",
    "        \n",
    "final_results2 = []\n",
    "for row in results2:\n",
    "    result_row = []\n",
    "    parent = row[0]\n",
    "    childrens = row[1]\n",
    "    for i in range(len(childrens)):\n",
    "        weight = (len(childrens) - i) / len(childrens)\n",
    "        result_row.append([parent, childrens[i], weight])\n",
    "    final_results2.extend(result_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "short-senegal",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = final_results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "hidden-mountain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at test 0.09599825739860535\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    s1t, s2t, rt = zip(*test)\n",
    "    stest_ = []\n",
    "    stest__ = []\n",
    "    for s in s1t:\n",
    "        sentense = nltk.tokenize.word_tokenize(s)\n",
    "        stest_.append(torch.tensor([word_to_ix[sentense[w]] if w < len(sentense) else 0 for w in range(50)]))\n",
    "    for s in s2t:\n",
    "        sentense = nltk.tokenize.word_tokenize(s)\n",
    "        stest__.append(torch.tensor([word_to_ix[sentense[w]] if w < len(sentense) else 0 for w in range(50)]))\n",
    "    \n",
    "    out = net(torch.stack(stest_).to(device), torch.stack(stest__).to(device)).squeeze()\n",
    "    loss = criterion(torch.tensor(rt, dtype=torch.float).to(device), out)\n",
    "    print(\"Loss at test {}\".format( loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "stylish-beginning",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_train = df_train.groupby(['repo']).agg(lambda x: list(x))\n",
    "df3_train['len'] = df3_train['iid'].map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cutting-great",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_train = df3_train[df3_train['len'] >= 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "breeding-district",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def predict(puzzle, currently_existed_puzzles):\n",
    "    with torch.no_grad():\n",
    "        sentense = nltk.tokenize.word_tokenize(puzzle)\n",
    "        getter = operator.itemgetter(*sentense)\n",
    "        len_v = min(len(sentense), 50)\n",
    "        vector = getter(word_to_ix)[:len_v]\n",
    "        vector_puzzle = torch.tensor(np.concatenate([vector, np.zeros(50 - len_v)], axis = 0).astype(np.int))\n",
    "        stest__ = []\n",
    "        for p in currently_existed_puzzles:\n",
    "            sentense = nltk.tokenize.word_tokenize(p)\n",
    "            getter = operator.itemgetter(*sentense)\n",
    "            len_v = min(50, len(sentense))\n",
    "            vector = getter(word_to_ix)[:len_v]\n",
    "            v=np.concatenate([vector, np.zeros(50 - len_v)], axis = 0).astype(np.int)\n",
    "            stest__.append(torch.tensor(v))\n",
    "        \n",
    "        out = net(vector_puzzle.repeat(len(stest__), 1).to(device), torch.stack(stest__).to(device)).squeeze()\n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "automatic-equity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_window_test(puzzles,n = 10):\n",
    "    puzzles = np.array(puzzles)\n",
    "    x = puzzles[np.lib.stride_tricks.sliding_window_view(np.arange(len(puzzles)), n)]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "automotive-cleveland",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-32-72024f3b8db1>:9: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  vector_puzzle = torch.tensor(np.concatenate([vector, np.zeros(50 - len_v)], axis = 0).astype(np.int))\n",
      "<ipython-input-32-72024f3b8db1>:16: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  v=np.concatenate([vector, np.zeros(50 - len_v)], axis = 0).astype(np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean average precision@5 for all repos with more than 7 commits on test data: 0.476679670567837\n"
     ]
    }
   ],
   "source": [
    "def ap_i(elems, top_k=6):\n",
    "    ap = 0\n",
    "    positives = 0\n",
    "    for i in range(len(elems)):\n",
    "        if elems[i] < top_k:\n",
    "            positives += 1\n",
    "            ap = positives / (i + 1)\n",
    "    return ap\n",
    "    \n",
    "aps = []\n",
    "for idx, row in df3_train.iterrows():\n",
    "    try:\n",
    "        window = create_window_test(row['text'], 20)\n",
    "        for row in window:\n",
    "            current_puzzle = row[0]\n",
    "            relevant = row[1:6]\n",
    "            nonrelevant = row[5:]\n",
    "            pred_r = predict(current_puzzle, relevant).cpu().detach().numpy()\n",
    "            pred_n = predict(current_puzzle, nonrelevant).cpu().detach().numpy()\n",
    "            try:\n",
    "                rq = np.concatenate((pred_r, pred_n), axis = 0)\n",
    "                res = np.argsort(-rq)\n",
    "                ap5 = ap_i(res[:5])\n",
    "                aps.append(ap5)\n",
    "            except:\n",
    "                pass\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "        \n",
    "print(f'Mean average precision@5 for all repos with more than 7 commits on test data: {np.mean(aps)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "romantic-russell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently solved puzzle is \n",
      "2out.h:13-13 place classes files the 5-03512da4 include/2out.h //github.com/dronmdf/2out/blob/master/include/2out.h lines 13-13 resolved place classes files `` the created andrey valyaev 22-may-17 estimate 15 minutes role imp if technical questions ask submit the done `` fixed _removed_ source here pdd //www.yegor256.com/2009/03/04/pdd.html //www.yegor256.com/2017/04/05/pdd-in-action.html,\n",
      "\n",
      " next five predicted puzzles:\n",
      "\n",
      " 0. tstsimple.cpp:21-22 need separate errro/failure result ... the 103-ee5b9b79 src/tstsimple.cpp //github.com/dronmdf/2out/blob/master/src/tstsimple.cpp lines 21-22 resolved need separate errro/failure result error assertion fail failure illegal exit `` the created andrey valyaev 15-jun-17 estimate 15 minutes role imp if technical questions ask submit the done `` fixed _removed_ source here pdd //www.yegor256.com/2009/03/04/pdd.html //www.yegor256.com/2017/04/05/pdd-in-action.html\n",
      "\n",
      "1. textreporttest.cpp:61-62 this test little fake failure ... the 46-0ce8ced1 test/textreporttest.cpp //github.com/dronmdf/2out/blob/master/test/textreporttest.cpp lines 61-62 resolved this test little fake failure output test name status `` the created andrey valyaev 30-may-17 estimate 15 minutes role imp if technical questions ask submit the done `` fixed _removed_ source here pdd //www.yegor256.com/2009/03/04/pdd.html //www.yegor256.com/2017/04/05/pdd-in-action.html\n",
      "\n",
      "2. textreporttest.cpp:45-46 this test little fake success ... the 46-f72a61c2 test/textreporttest.cpp //github.com/dronmdf/2out/blob/master/test/textreporttest.cpp lines 45-46 resolved this test little fake success output test name status `` the created andrey valyaev 30-may-17 estimate 15 minutes role imp if technical questions ask submit the done `` fixed _removed_ source here pdd //www.yegor256.com/2009/03/04/pdd.html //www.yegor256.com/2017/04/05/pdd-in-action.html\n",
      "\n",
      "3. cmakelists.txt:9-11 remove include directories root .... the 223-d2f13a0f cmakelists.txt //github.com/dronmdf/2out/blob/master/cmakelists.txt lines 9-11 resolved remove include directories root test reference youself root directory include directory 2out need specify include directory `` the created andrey valyaev 03-jul-17 estimate 15 minutes role imp if technical questions ask submit the done `` fixed _removed_ source here pdd //www.yegor256.com/2009/03/04/pdd.html //www.yegor256.com/2017/04/05/pdd-in-action.html\n",
      "\n",
      "4. representation.h:11-12 need introduce generic ... the 151-ddec6706 include/representation.h //github.com/dronmdf/2out/blob/master/include/representation.h lines 11-12 resolved need introduce generic representation type this repr use ostringstream stringize object `` the created andrey valyaev 26-jun-17 estimate 15 minutes role imp if technical questions ask submit the done `` fixed _removed_ source here pdd //www.yegor256.com/2009/03/04/pdd.html //www.yegor256.com/2017/04/05/pdd-in-action.html\n",
      "------------------------------------\n",
      "Currently solved puzzle is \n",
      "main.cpp:21 make inventory request parse reply the 4-e8896619 4 resolved //github.com/dronmdf/doors/blob/2123a7b5262a8467cc2fa7d1e2d6f728da8b2326/main.cpp l21-l21 the created andrey valyaev 20-feb-19 if technical questions ask submit the `` done\\ `` fixed _removed_ source here pdd //www.yegor256.com/2009/03/04/pdd.html //www.yegor256.com/2017/04/05/pdd-in-action.html,\n",
      "\n",
      " next five predicted puzzles:\n",
      "\n",
      " 0. main.cpp:108 разобрать status ответ от сервера the 24-c4e3931f 24 resolved //github.com/dronmdf/doors/blob/d5e5b5eef5d01b8a6eb0026457186e26f979af3c/util/main.cpp l108-l108 the created andrey valyaev 09-mar-19 if technical questions ask submit the `` done\\ `` fixed _removed_ source here pdd //www.yegor256.com/2009/03/04/pdd.html //www.yegor256.com/2017/04/05/pdd-in-action.html\n",
      "\n",
      "1. asioscheduler.cpp:21-23 если delay определен операцию ... the 113-8b114594 113 resolved //github.com/dronmdf/doors/blob/bb5e464084d22d5315ac577b26d85e6250c49d8b/core/asioscheduler.cpp l21-l23 the created andrey valyaev 18-may-19 if technical questions ask submit the `` done\\ `` fixed _removed_ source here pdd //www.yegor256.com/2009/03/04/pdd.html //www.yegor256.com/2017/04/05/pdd-in-action.html\n",
      "\n",
      "2. httpservice.h:18-19 storagehandler не самый подходящий ... the 95-589bb57b 95 resolved //github.com/dronmdf/doors/blob/54ff6021f597b9197bee91dccbb2783ae67ff951/core/httpservice.h l18-l19 the created andrey valyaev 25-may-19 if technical questions ask submit the `` done\\ `` fixed _removed_ source here pdd //www.yegor256.com/2009/03/04/pdd.html //www.yegor256.com/2017/04/05/pdd-in-action.html\n",
      "\n",
      "3. urls.py:22-25 создать точку входа для обновления в ... the 72-eeea2729 72 resolved //github.com/dronmdf/doors/blob/3c5f2807ba4b83e41de3ac92013bcf0e333d43f0/web/web/urls.py l22-l25 the created andrey valyaev 17-apr-19 if technical questions ask submit the `` done\\ `` fixed _removed_ source here pdd //www.yegor256.com/2009/03/04/pdd.html //www.yegor256.com/2017/04/05/pdd-in-action.html\n",
      "\n",
      "4. main.cpp:32 добавить http хранилище the 64-c7c642e4 64 resolved //github.com/dronmdf/doors/blob/1bda63293140cc5baf62f69066324ed4f423a9d1/server/main.cpp l32-l32 the created andrey valyaev 15-apr-19 if technical questions ask submit the `` done\\ `` fixed _removed_ source here pdd //www.yegor256.com/2009/03/04/pdd.html //www.yegor256.com/2017/04/05/pdd-in-action.html\n",
      "------------------------------------\n",
      "Currently solved puzzle is \n",
      "test_report.py:43-45 test_result hide detailes test ... the 63-5bbfe472 test_report.py //github.com/dronmdf/goost/blob/master/test_report.py lines 43-45 resolved test_result hide detailes test failure this may build machine specific environment shippable display failure details log the created andrey valyaev 07-apr-17 estimate 0 minutes role imp if technical questions ask submit the done `` fixed removed source here pdd //www.yegor256.com/2009/03/04/pdd.html,\n",
      "\n",
      " next five predicted puzzles:\n",
      "\n",
      " 0. style.sh:5-5 add benchmark dir style check ... the 340-2e4712dc 340 style.sh //github.com/dronmdf/goost/blob/master/style.sh lines 5-5 resolved add benchmark dir style check rework `` the created andrey valyaev 09-nov-17 estimate 0 minutes role imp if technical questions ask submit the done `` fixed _removed_ source here pdd //www.yegor256.com/2009/03/04/pdd.html //www.yegor256.com/2017/04/05/pdd-in-action.html\n",
      "\n",
      "1. test_report.py:49-50 failures xml file without details ... the 63-76f37125 test_report.py //github.com/dronmdf/goost/blob/master/test_report.py lines 49-50 resolved failures xml file without details details stored system-out/ tag the created andrey valyaev 07-apr-17 estimate 0 minutes role imp if technical questions ask submit the done `` fixed removed source here pdd //www.yegor256.com/2009/03/04/pdd.html\n",
      "\n",
      "2. operations.h:7-8 remove operations utility ... the 59-489625a6 kuznyechik/operations.h //github.com/dronmdf/goost/blob/master/kuznyechik/operations.h lines 7-8 resolved remove operations utility class need introduce small classes transformed blocks the created andrey valyaev 09-apr-17 estimate 30 minutes role imp if technical questions ask submit the done `` fixed removed source here pdd //www.yegor256.com/2009/03/04/pdd.html\n",
      "\n",
      "3. keyiter.cpp:46-47 cn const key need predefine ... the 82-68bf8978 kuznyechik/keyiter.cpp //github.com/dronmdf/goost/blob/master/kuznyechik/keyiter.cpp lines 46-47 resolved cn const key need predefine keys the created andrey valyaev 20-apr-17 estimate 30 minutes role imp if technical questions ask submit the done `` fixed removed source here pdd //www.yegor256.com/2009/03/04/pdd.html\n",
      "\n",
      "4. encryptedblocktest.cpp:35-36 extract key fixture fixture ... the 139-02d6b4f9 139 test2o/kuznyechik/encryptedblocktest.cpp //github.com/dronmdf/goost/blob/master/test2o/kuznyechik/encryptedblocktest.cpp lines 35-36 resolved extract key fixture fixture conception complete can try `` the created andrey valyaev 16-aug-17 estimate 15 minutes role imp if technical questions ask submit the done `` fixed _removed_ source here pdd //www.yegor256.com/2009/03/04/pdd.html //www.yegor256.com/2017/04/05/pdd-in-action.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-32-72024f3b8db1>:9: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  vector_puzzle = torch.tensor(np.concatenate([vector, np.zeros(50 - len_v)], axis = 0).astype(np.int))\n",
      "<ipython-input-32-72024f3b8db1>:16: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  v=np.concatenate([vector, np.zeros(50 - len_v)], axis = 0).astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "n_line = \"\\n\\n\"\n",
    "num_ind, str_ind = zip(*enumerate(list(df3.index)))\n",
    "ind = list(set(random.choices(num_ind,k=10)))\n",
    "df3 = df3.iloc[ind]\n",
    "\n",
    "aps = []\n",
    "i = 0\n",
    "for idx, row in df3_train.iterrows():\n",
    "    current_puzzle = row['text'][0]\n",
    "    relevant = row['text'][1:6]\n",
    "    nonrelevant = row['text'][5:]\n",
    "    pred_r = predict(current_puzzle, relevant).cpu().detach().numpy()\n",
    "    pred_n = predict(current_puzzle, nonrelevant).cpu().detach().numpy()\n",
    "    strings = np.concatenate((relevant, nonrelevant), axis = 0)\n",
    "    r = np.concatenate((pred_r, pred_n), axis=0)\n",
    "    res = np.argsort(-r)\n",
    "    results = n_line.join([f\"{i}. {s}\" for i,s in enumerate(strings[res][:5])])\n",
    "    print(f\"Currently solved puzzle is \\n{current_puzzle},\\n\\n next five predicted puzzles:\\n\\n {results}\")\n",
    "    i += 1 \n",
    "    if i == 3:\n",
    "        break\n",
    "    else:\n",
    "        print('------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cardiac-interim",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_test = df_test.groupby(['repo']).agg(lambda x: list(x))\n",
    "df3_test['len'] = df3_test['iid'].map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cheap-airline",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_test = df3_test[df3_test['len'] >= 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "million-theta",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 5)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "apparent-sampling",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-32-72024f3b8db1>:9: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  vector_puzzle = torch.tensor(np.concatenate([vector, np.zeros(50 - len_v)], axis = 0).astype(np.int))\n",
      "<ipython-input-32-72024f3b8db1>:16: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  v=np.concatenate([vector, np.zeros(50 - len_v)], axis = 0).astype(np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean average precision@5 for all repos with more than 7 commits on test data: 0.5956709956709956\n"
     ]
    }
   ],
   "source": [
    "aps = []\n",
    "for idx, row in df3_test.iterrows():\n",
    "    try:\n",
    "        window = create_window_test(row['text'],15)\n",
    "        for row in window:\n",
    "            current_puzzle = row[0]\n",
    "            relevant = row[1:6]\n",
    "            nonrelevant = row[5:]\n",
    "            pred_r = predict(current_puzzle, relevant).cpu().detach().numpy()\n",
    "            pred_n = predict(current_puzzle, nonrelevant).cpu().detach().numpy()\n",
    "            try:\n",
    "                rq = np.concatenate((pred_r, pred_n), axis = 0)\n",
    "                res = np.argsort(-rq)\n",
    "                ap5 = ap_i(res[:5])\n",
    "                aps.append(ap5)\n",
    "            except:\n",
    "                pass\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "print(f'Mean average precision@5 for all repos with more than 7 commits on test data: {np.mean(aps)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "written-laundry",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3320/3320 [00:06<00:00, 539.98it/s] \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('enriched-extended-issues-with-relationship.json','r') as f:\n",
    "    data = json.loads(f.read())\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = ['to','the','for', 'this','a', 'and','of','is','we','be','pdd','com', 'puzzle','in','on','by',\n",
    "             'has', 'was','me', 'http','html','about','yegor','www','from','java','github','new','have','https',\n",
    "              'code','will','instead','any','don','text','task','more','when','you', 'problem','tickets'\n",
    "            ]\n",
    "\n",
    "def get_row(issue):\n",
    "    def _(f):\n",
    "        return issue[f] if f in issue else None\n",
    "    \n",
    "    def _t(f1, f2):\n",
    "        return (parse(_(f2)) - parse(_(f1))).total_seconds() if _(f1) and _(f2) else None \n",
    "    \n",
    "    def __(f1, f2):\n",
    "        return _(f1)[f2] if _(f1) and f2 in _(f1) else None\n",
    "    \n",
    "    def _l():\n",
    "        return [label['name'] for label in _('labels')] if _('labels') else []\n",
    "    \n",
    "    def clean(x):\n",
    "        return ' '.join([word for word in word_tokenize(x) if word not in stop_words])\n",
    "    \n",
    "    title = _('title')\n",
    "    body = _('body')\n",
    "    issue_id = _('id')\n",
    "    child_id = __('parentIssue','id')\n",
    "    text = f'{clean(title)} {clean(body)}'\n",
    "    if text and not text.isspace():  \n",
    "        return [issue_id, child_id, text]\n",
    "    else:\n",
    "        return [None,None,None]\n",
    "overall_data = []\n",
    "\n",
    "overall_data.extend([get_row(issue) for issue in data])\n",
    "df = pd.DataFrame(overall_data)\n",
    "df.columns = ['iid', 'pid', 'text']\n",
    "df2 = df.dropna(subset=['pid'])\n",
    "all_puzzles = df2['iid'].values + df2['pid'].values\n",
    "all_puzzles = list(df2['iid'].values)+ list(df2['pid'].values)\n",
    "def get_depth(iid):\n",
    "    i = 0\n",
    "    while len(df2[df2['pid'] == iid].values) != 0:\n",
    "        i += 1\n",
    "        iid2 = df2[df2['pid'] == iid].values[0][0]\n",
    "        if iid2 == iid:\n",
    "            break\n",
    "        else:\n",
    "            iid = iid2\n",
    "    return i\n",
    "child_counts = {}\n",
    "from tqdm import tqdm\n",
    "for row in tqdm(all_puzzles):\n",
    "    child_counts[row] = get_depth(row)\n",
    "ds = []\n",
    "for k,v in child_counts.items():\n",
    "    ds.append([k,v])\n",
    "df3 = pd.DataFrame(ds)\n",
    "df3.columns = ['iid','count']\n",
    "df4 = pd.merge(df3,df2, on='iid')[['iid','count','text']]\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(df4['text'].values)]\n",
    "model = Doc2Vec(documents, vector_size=10, window=2, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "together-creativity",
   "metadata": {},
   "outputs": [],
   "source": [
    "res4 = []\n",
    "for idx,row in df4.iterrows():\n",
    "    res4.append([row['iid'],row['count'], model.infer_vector(row['text'].split())])\n",
    "    \n",
    "df5 = pd.DataFrame(res4)\n",
    "df5.columns = ['iid','count','text']\n",
    "df5 = df5.drop(df5[df5['count'] > 9].index)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "X = df5['text']\n",
    "y = df5['count']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.stack(X.values), y.values, test_size=0.33, random_state=42, stratify = y.values)\n",
    "\n",
    "model_rf = RandomForestRegressor(n_estimators=500, oob_score=True, random_state=100)\n",
    "model_rf.fit(X_train, y_train) \n",
    "# from sklearn.cluster import MeanShift\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "import numpy as np\n",
    "    \n",
    "def top_n_indexes(arr, n):\n",
    "    idx = np.argpartition(arr, arr.size-n, axis=None)[-n:]\n",
    "    width = arr.shape[1]\n",
    "    return [divmod(i, width) for i in idx]\n",
    "\n",
    "def get_first_unique(values):\n",
    "    res = {}\n",
    "    for idx, value in enumerate(values):\n",
    "        if value not in res:\n",
    "            res[value] = idx\n",
    "    return np.array(list(res.values()))\n",
    "\n",
    "def get_best_task(tasks):\n",
    "    X_text = tasks\n",
    "    X_vector = []\n",
    "    for task in X_text:\n",
    "        X_vector.append(model.infer_vector(task.split()))\n",
    "    X_vector = np.stack(X_vector)\n",
    "    predictions = model_rf.predict(X_vector)\n",
    "    best_tasks = np.argsort(predictions)[:1000]\n",
    "    clustering = KMedoids(n_clusters=5, random_state=42).fit(X_vector[best_tasks]).labels_\n",
    "    unique_indecies = get_first_unique(clustering)\n",
    "    return X_text[best_tasks[clustering[unique_indecies]]], X_vector[best_tasks[clustering[unique_indecies]]], X_vector, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "allied-alcohol",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ap_i__(predicted, relevant):\n",
    "    relevant = set(relevant)\n",
    "    ap = 0\n",
    "    positives = 0\n",
    "    for i in range(len(predicted)):\n",
    "        if predicted[i] in relevant:\n",
    "            positives += 1\n",
    "            ap = positives / (i + 1)\n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "adjacent-bookmark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean average precision@5 for all repos with more than 7 commits on train data: 0.41008911521324\n"
     ]
    }
   ],
   "source": [
    "aps = []\n",
    "for idx, row in df3_train.iterrows():\n",
    "    window = create_window_test(row['text'], min(20,len(row['text'])) )\n",
    "    for row in window:\n",
    "        current_puzzle = row[0]\n",
    "        relevant = row[1:6]\n",
    "        nonrelevant = row[5:]\n",
    "        pred_r = get_best_task(row)[0]\n",
    "        aps.append(ap_i__(pred_r, relevant))\n",
    "\n",
    "print(f'Mean average precision@5 for all repos with more than 7 commits on train data: {np.mean(aps)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "prerequisite-television",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean average precision@5 for all repos with more than 7 commits on train data: 0.39748427672955966\n"
     ]
    }
   ],
   "source": [
    "aps = []\n",
    "for idx, row in df3_test.iterrows():\n",
    "    window = create_window_test(row['text'], min(20,len(row['text'])) )\n",
    "    for row in window:\n",
    "        current_puzzle = row[0]\n",
    "        relevant = row[1:6]\n",
    "        nonrelevant = row[5:]\n",
    "        pred_r = get_best_task(row)[0]\n",
    "        aps.append(ap_i__(pred_r, relevant))\n",
    "\n",
    "print(f'Mean average precision@5 for all repos with more than 7 commits on train data: {np.mean(aps)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
